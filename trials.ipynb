{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.3.16-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\asus\\anaconda3\\install\\envs\\mchatbot\\lib\\site-packages (from langchain-community) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\asus\\anaconda3\\install\\envs\\mchatbot\\lib\\site-packages (from langchain-community) (2.0.37)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\asus\\anaconda3\\install\\envs\\mchatbot\\lib\\site-packages (from langchain-community) (3.11.11)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\asus\\anaconda3\\install\\envs\\mchatbot\\lib\\site-packages (from langchain-community) (0.5.14)\n",
      "Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain-community)\n",
      "  Using cached httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: langchain<0.4.0,>=0.3.16 in c:\\users\\asus\\anaconda3\\install\\envs\\mchatbot\\lib\\site-packages (from langchain-community) (0.3.17)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.32 in c:\\users\\asus\\anaconda3\\install\\envs\\mchatbot\\lib\\site-packages (from langchain-community) (0.3.33)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in c:\\users\\asus\\anaconda3\\install\\envs\\mchatbot\\lib\\site-packages (from langchain-community) (0.3.3)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in c:\\users\\asus\\anaconda3\\install\\envs\\mchatbot\\lib\\site-packages (from langchain-community) (1.26.4)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
      "  Using cached pydantic_settings-2.7.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\asus\\anaconda3\\install\\envs\\mchatbot\\lib\\site-packages (from langchain-community) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\asus\\anaconda3\\install\\envs\\mchatbot\\lib\\site-packages (from langchain-community) (8.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\asus\\anaconda3\\install\\envs\\mchatbot\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\asus\\anaconda3\\install\\envs\\mchatbot\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\asus\\anaconda3\\install\\envs\\mchatbot\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\asus\\anaconda3\\install\\envs\\mchatbot\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\asus\\anaconda3\\install\\envs\\mchatbot\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\asus\\anaconda3\\install\\envs\\mchatbot\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\asus\\anaconda3\\install\\envs\\mchatbot\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\asus\\anaconda3\\install\\envs\\mchatbot\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.0)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\asus\\anaconda3\\install\\envs\\mchatbot\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in c:\\users\\asus\\anaconda3\\install\\envs\\mchatbot\\lib\\site-packages (from langchain<0.4.0,>=0.3.16->langchain-community) (0.3.5)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\asus\\anaconda3\\install\\envs\\mchatbot\\lib\\site-packages (from langchain<0.4.0,>=0.3.16->langchain-community) (2.10.6)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\asus\\anaconda3\\install\\envs\\mchatbot\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.32->langchain-community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\asus\\anaconda3\\install\\envs\\mchatbot\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.32->langchain-community) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\asus\\anaconda3\\install\\envs\\mchatbot\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.32->langchain-community) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\asus\\anaconda3\\install\\envs\\mchatbot\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\asus\\anaconda3\\install\\envs\\mchatbot\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\asus\\anaconda3\\install\\envs\\mchatbot\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\asus\\anaconda3\\install\\envs\\mchatbot\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
      "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
      "  Using cached python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus\\anaconda3\\install\\envs\\mchatbot\\lib\\site-packages (from requests<3,>=2->langchain-community) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\anaconda3\\install\\envs\\mchatbot\\lib\\site-packages (from requests<3,>=2->langchain-community) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\anaconda3\\install\\envs\\mchatbot\\lib\\site-packages (from requests<3,>=2->langchain-community) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\anaconda3\\install\\envs\\mchatbot\\lib\\site-packages (from requests<3,>=2->langchain-community) (2025.1.31)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\asus\\anaconda3\\install\\envs\\mchatbot\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\asus\\anaconda3\\install\\envs\\mchatbot\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (4.8.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\asus\\anaconda3\\install\\envs\\mchatbot\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\asus\\anaconda3\\install\\envs\\mchatbot\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\asus\\anaconda3\\install\\envs\\mchatbot\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.32->langchain-community) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\asus\\anaconda3\\install\\envs\\mchatbot\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.16->langchain-community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\asus\\anaconda3\\install\\envs\\mchatbot\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.16->langchain-community) (2.27.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\asus\\anaconda3\\install\\envs\\mchatbot\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\asus\\anaconda3\\install\\envs\\mchatbot\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n",
      "Downloading langchain_community-0.3.16-py3-none-any.whl (2.5 MB)\n",
      "   ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n",
      "   -------------------- ------------------- 1.3/2.5 MB 8.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.5/2.5 MB 8.0 MB/s eta 0:00:00\n",
      "Using cached httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Using cached pydantic_settings-2.7.1-py3-none-any.whl (29 kB)\n",
      "Using cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: python-dotenv, httpx-sse, pydantic-settings, langchain-community\n",
      "Successfully installed httpx-sse-0.4.0 langchain-community-0.3.16 pydantic-settings-2.7.1 python-dotenv-1.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\anaconda3\\install\\envs\\mchatbot\\Lib\\site-packages\\pinecone\\index.py:4: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "import pinecone\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import CTransformers\n",
    "from tqdm.autonotebook import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_API_KEY = \"pcsk_2o8g2_F1QiRhz5R9HdvmjmfnrYByTPtY1BUFFED3JAtcs13SgjYdqfJYhcNbbAexhgCQx\"\n",
    "PINECONE_API_ENV = \"gcp-starter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract data from the PDF\n",
    "def load_pdf(data):\n",
    "    loader = DirectoryLoader(data,\n",
    "                    glob=\"*.pdf\",\n",
    "                    loader_cls=PyPDFLoader)\n",
    "    \n",
    "    documents = loader.load()\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_data = load_pdf(\"data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create text chunks\n",
    "def text_split(extracted_data):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 20)\n",
    "    text_chunks = text_splitter.split_documents(extracted_data)\n",
    "\n",
    "    return text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of my chunk: 5860\n"
     ]
    }
   ],
   "source": [
    "text_chunks = text_split(extracted_data)\n",
    "print(\"length of my chunk:\", len(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download embedding model\n",
    "def download_hugging_face_embeddings():\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.9745e-01,  1.7767e-01,  3.8570e-02,  1.4952e-01, -2.2542e-01,\n",
      "         -9.1803e-01,  3.8326e-01, -3.6890e-02, -2.7174e-01,  8.4522e-02,\n",
      "          4.0589e-01,  3.1800e-01,  1.0992e-01, -1.5034e-01, -5.7896e-02,\n",
      "         -1.5428e-01,  1.2775e-01, -1.2729e-01, -8.5727e-01, -1.0018e-01,\n",
      "          4.3961e-02,  3.1127e-01,  1.8638e-02,  1.8169e-01, -4.8461e-01,\n",
      "         -1.6840e-01,  2.9548e-01,  2.7560e-01, -1.8983e-02, -3.3376e-01,\n",
      "          2.4035e-01,  1.2720e-01,  7.3412e-01, -1.2793e-01, -6.6753e-02,\n",
      "          3.6038e-01, -1.8828e-01, -5.2244e-01, -1.7854e-01,  3.0180e-01,\n",
      "          2.6936e-01, -4.8222e-01, -1.7213e-01, -1.1880e-01,  5.4508e-02,\n",
      "         -2.1314e-02,  4.2054e-02,  2.2520e-01,  5.3417e-01, -2.1697e-02,\n",
      "         -3.0205e-01, -3.3249e-01, -3.9311e-02,  3.0257e-02,  4.7472e-01,\n",
      "          1.1089e-01,  3.5991e-02, -5.9163e-02,  5.1727e-02, -2.1581e-01,\n",
      "         -2.5889e-01,  1.3754e-01, -3.9768e-02,  7.7265e-02,  5.7300e-01,\n",
      "         -4.1052e-01, -1.2424e-01,  1.8107e-01, -2.9570e-01, -4.7102e-01,\n",
      "         -3.7622e-01, -5.6669e-02,  3.3310e-02,  4.2124e-01, -1.9500e-01,\n",
      "          1.4252e-01,  8.2971e-02,  1.5152e-01,  5.5303e-02,  1.7305e-01,\n",
      "          3.0240e-01, -4.3157e-01,  5.6680e-02,  1.7087e-01,  1.0054e-01,\n",
      "          1.3224e-01,  1.1075e-02,  8.0188e-03, -2.7017e-01, -6.4108e-02,\n",
      "         -6.5401e-01, -1.1346e-01,  2.3060e-01,  1.2559e-02, -4.5696e-01,\n",
      "         -1.4536e-01,  5.4109e-01, -1.6597e-01, -8.3041e-01,  1.3228e+00,\n",
      "          1.5881e-01,  1.8390e-01,  1.7790e-01,  2.4530e-01,  3.6788e-01,\n",
      "          1.8419e-01, -2.7928e-02,  3.1898e-01, -2.1494e-01, -1.2316e-01,\n",
      "         -1.6231e-01, -1.6520e-01,  2.1964e-01, -1.0004e-01,  3.0058e-01,\n",
      "         -4.2880e-01, -1.7902e-01,  1.2508e-01, -2.2848e-01, -4.9177e-02,\n",
      "          1.5438e-01, -2.7773e-01,  6.5686e-02,  1.6962e-01, -1.1781e-01,\n",
      "          7.5044e-02,  1.6513e-01, -1.8293e-32,  3.7100e-01, -1.0383e-01,\n",
      "          2.9659e-01,  6.9858e-01,  1.6482e-01,  4.9950e-02, -4.0386e-01,\n",
      "         -9.6825e-02,  2.3331e-01,  2.4119e-01,  1.4573e-01,  2.0471e-01,\n",
      "         -2.8144e-01,  1.2193e-02, -8.9033e-02,  2.9053e-01, -2.7595e-01,\n",
      "          2.0548e-01, -2.3291e-02,  5.8256e-01, -3.2053e-01, -6.1169e-02,\n",
      "          6.4345e-02,  5.1935e-01,  2.4250e-02,  2.0123e-01, -5.5567e-02,\n",
      "         -5.3755e-01,  5.3177e-01,  4.5843e-02, -4.4128e-02, -2.9829e-01,\n",
      "         -7.2089e-02,  1.8709e-02,  3.4439e-02,  4.3418e-02,  6.0230e-02,\n",
      "         -4.9449e-01, -4.0019e-01, -1.4511e-02, -5.2101e-01,  2.6852e-01,\n",
      "          2.9823e-01,  4.1198e-02,  6.2444e-02, -2.9949e-02,  7.9817e-02,\n",
      "          1.2581e-01,  1.9591e-01,  3.4490e-01,  6.6818e-04,  8.4368e-02,\n",
      "         -4.0139e-01,  1.6321e-01, -1.5807e-01,  6.1669e-02,  1.9947e-01,\n",
      "         -1.2878e-01,  5.5946e-02,  4.4227e-01,  1.2363e-01,  6.5834e-01,\n",
      "         -3.8943e-01,  1.3608e-01, -9.1537e-02, -1.0209e-01,  3.6878e-01,\n",
      "          1.8341e-01,  2.8789e-01, -3.3867e-02, -1.9304e-01,  1.0217e-01,\n",
      "          9.4913e-02,  3.6249e-01,  1.9859e-01,  2.6615e-01,  5.6069e-01,\n",
      "         -3.8001e-02,  1.4436e-01, -4.4663e-01,  9.6935e-02, -5.4166e-03,\n",
      "          1.2869e-01, -2.1907e-01,  5.4809e-01, -3.0643e-02,  5.9955e-02,\n",
      "         -6.5997e-01, -7.5952e-02, -6.1331e-02, -4.7600e-01,  4.1963e-01,\n",
      "          2.8286e-01, -5.1509e-02, -5.4889e-01,  1.9277e-32,  7.1547e-01,\n",
      "          1.1081e-01, -3.3345e-01, -2.0610e-01, -2.9062e-01, -2.6150e-01,\n",
      "         -4.7306e-01,  8.4869e-01, -5.0637e-01,  3.4518e-01,  2.9224e-01,\n",
      "          5.9005e-02,  8.0871e-01,  1.7647e-01,  3.4953e-01, -3.0267e-01,\n",
      "          7.8257e-01,  5.2629e-02, -9.9219e-02, -7.3582e-02, -4.5788e-02,\n",
      "         -2.9196e-01, -2.9980e-01,  4.3484e-02, -8.6855e-02,  9.7129e-02,\n",
      "          1.2181e-01,  1.1773e-01, -6.8738e-01,  8.2821e-02,  1.5325e-01,\n",
      "          1.4506e-01, -2.4485e-01,  3.8762e-02, -8.2802e-02,  2.5921e-01,\n",
      "         -5.2387e-01, -1.1132e-01, -1.0213e-01, -3.1446e-01, -3.0147e-01,\n",
      "         -5.9898e-02, -2.9789e-01,  1.1965e-01, -4.5798e-01, -6.9360e-02,\n",
      "         -3.3062e-01,  1.3274e-01, -4.5996e-02, -1.4884e-01, -4.5790e-01,\n",
      "         -1.1871e-01,  2.7957e-01, -1.1677e-01, -2.8163e-01,  8.1090e-02,\n",
      "         -3.6435e-01, -4.4712e-02,  9.4101e-02, -1.4708e-01,  7.6632e-02,\n",
      "          1.5032e-01,  5.7145e-02,  3.6210e-01,  1.5303e-02, -3.7698e-02,\n",
      "          9.5249e-02,  1.8536e-01,  2.1729e-01, -2.0832e-01, -3.9578e-02,\n",
      "          9.1439e-04, -9.3551e-03, -1.5622e-01, -1.6057e-01,  2.8452e-01,\n",
      "         -1.6532e-01, -1.3848e-02,  8.4613e-02,  5.5920e-02,  3.3202e-02,\n",
      "          7.7233e-02,  3.1887e-02,  2.1319e-01,  4.1419e-02,  2.2997e-01,\n",
      "          4.6676e-01,  4.1229e-01, -7.4771e-02, -2.4558e-01, -6.3060e-02,\n",
      "          2.8049e-02, -5.2857e-02,  2.0154e-01, -2.9227e-01, -8.9994e-08,\n",
      "         -5.0754e-01,  1.3693e-01, -9.2997e-02,  1.8154e-01,  1.5625e-01,\n",
      "          3.0048e-01, -2.6957e-01, -3.3701e-01, -3.6198e-01,  2.3416e-01,\n",
      "          2.8536e-01,  6.1021e-01, -4.2666e-01, -7.1559e-02,  1.0521e-01,\n",
      "          2.2606e-01, -1.4201e-01,  8.3132e-02, -2.1229e-01,  1.1463e-01,\n",
      "         -2.7810e-04,  5.6504e-02,  1.4225e-01, -3.0042e-01,  1.6788e-01,\n",
      "         -4.9933e-01, -8.3038e-02,  1.4901e-01, -1.0736e-01, -4.3642e-01,\n",
      "          2.0069e-01,  5.9353e-01, -1.6064e-01,  7.2835e-02, -4.3710e-01,\n",
      "         -1.0682e-01,  1.4304e-01,  4.6642e-01,  3.9377e-01, -3.6684e-01,\n",
      "         -4.8045e-01,  3.5141e-01, -1.9211e-01, -6.0792e-01, -2.2954e-01,\n",
      "          1.8630e-01,  4.3882e-01, -4.1815e-01,  1.9332e-03, -2.3407e-01,\n",
      "         -4.3403e-01,  1.5765e-01,  4.2737e-01,  1.0146e-01,  5.2239e-01,\n",
      "          6.3121e-01,  3.2630e-03,  2.9472e-01, -8.3333e-02,  1.9031e-01,\n",
      "          1.3625e-01, -1.3109e-01,  2.2299e-01,  1.7299e-01]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load pre-trained model and tokenizer from Hugging Face\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Example text\n",
    "sentences = [\"Hello World\"]\n",
    "inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Get embeddings\n",
    "with torch.no_grad():\n",
    "    embeddings = model(**inputs).last_hidden_state.mean(dim=1)\n",
    "\n",
    "print(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:\n",
      "Allergies are the body's immune response to substances.\n",
      "Treatment for allergies often involves antihistamines.\n",
      "Skin rashes may appear when someone has an allergy.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load pre-trained model and tokenizer from Hugging Face\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Example text (replace with your text chunks)\n",
    "text_chunks = [\n",
    "    {\"page_content\": \"Allergies are the body's immune response to substances.\"},\n",
    "    {\"page_content\": \"A headache can occur from various conditions, including allergies.\"},\n",
    "    {\"page_content\": \"Fever is a common symptom of infection.\"},\n",
    "    {\"page_content\": \"Skin rashes may appear when someone has an allergy.\"},\n",
    "    {\"page_content\": \"Treatment for allergies often involves antihistamines.\"}\n",
    "]\n",
    "\n",
    "# Step 1: Generate embeddings for the text chunks\n",
    "def generate_embeddings(text_chunks):\n",
    "    embeddings = []\n",
    "    for chunk in text_chunks:\n",
    "        sentence = chunk['page_content']\n",
    "        inputs = tokenizer(sentence, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "        # Get embeddings\n",
    "        with torch.no_grad():\n",
    "            embedding = model(**inputs).last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "        embeddings.append(embedding)\n",
    "    return np.array(embeddings)\n",
    "\n",
    "# Get embeddings for the text chunks\n",
    "embeddings = generate_embeddings(text_chunks)\n",
    "\n",
    "# Step 2: Perform similarity search\n",
    "def similarity_search(query, embeddings, text_chunks, top_k=3):\n",
    "    # Generate embedding for the query\n",
    "    inputs = tokenizer(query, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        query_embedding = model(**inputs).last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "    # Compute cosine similarities between query and document embeddings\n",
    "    similarities = cosine_similarity([query_embedding], embeddings)\n",
    "    \n",
    "    # Get indices of the top_k most similar documents\n",
    "    top_k_indices = similarities.argsort()[0][-top_k:][::-1]\n",
    "    \n",
    "    # Get the corresponding documents\n",
    "    results = [text_chunks[i] for i in top_k_indices]\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Step 3: Query\n",
    "query = \"What are allergies?\"\n",
    "results = similarity_search(query, embeddings, text_chunks)\n",
    "\n",
    "# Output the results\n",
    "print(\"Results:\")\n",
    "for result in results:\n",
    "    print(result['page_content'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded successfully on CPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\anaconda3\\install\\envs\\mchatbot\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ASUS\\anaconda3\\install\\envs\\mchatbot\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ASUS\\anaconda3\\install\\envs\\mchatbot\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:676: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:  \n",
      "    Answer the following question based on the context provided below. Provide a detailed, informative response. \n",
      "    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "    Context: Treatment for allergies often involves antihistamines.\n",
      "Allergies are the body's immune response to substances.\n",
      "Skin rashes may appear when someone has an allergy.\n",
      "    Question: Medicine for allergy\n",
      "    \n",
      "    Only return the helpful answer below and nothing else.\n",
      "    Helpful answer:\n",
      "     I have a history of allergies. I am allergic to peanuts, peanuts and other allergens. My doctor has told me that I should not use peanuts. If I do, I will not be able to use them. The doctor also told my family that they should be careful about using peanuts in their food. They have told them that if\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\anaconda3\\install\\envs\\mchatbot\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ASUS\\anaconda3\\install\\envs\\mchatbot\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ASUS\\anaconda3\\install\\envs\\mchatbot\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:676: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:  \n",
      "    Answer the following question based on the context provided below. Provide a detailed, informative response. \n",
      "    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "    Context: Skin rashes may appear when someone has an allergy.\n",
      "Allergies are the body's immune response to substances.\n",
      "Treatment for allergies often involves antihistamines.\n",
      "    Question: what are allergy?\n",
      "    \n",
      "    Only return the helpful answer below and nothing else.\n",
      "    Helpful answer:\n",
      "     The following is a list of the most common allergies. The list is not exhaustive. If there are any allergies, please let me know. I will try my best to help you. Please note that I am not a doctor. My experience is that most people with allergies are not allergic to anything. They are allergic only to the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\anaconda3\\install\\envs\\mchatbot\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ASUS\\anaconda3\\install\\envs\\mchatbot\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ASUS\\anaconda3\\install\\envs\\mchatbot\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:676: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:  \n",
      "    Answer the following question based on the context provided below. Provide a detailed, informative response. \n",
      "    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "    Context: Skin rashes may appear when someone has an allergy.\n",
      "Allergies are the body's immune response to substances.\n",
      "Treatment for allergies often involves antihistamines.\n",
      "    Question: What are allergy?\n",
      "    \n",
      "    Only return the helpful answer below and nothing else.\n",
      "    Helpful answer:\n",
      "     The following is a list of the most common allergies. The list is not exhaustive. If there are any allergies, please let me know. I will try my best to help you. Please note that I am not a doctor. My experience is that most people with allergies are not allergic to anything. They are allergic only to the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\anaconda3\\install\\envs\\mchatbot\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ASUS\\anaconda3\\install\\envs\\mchatbot\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ASUS\\anaconda3\\install\\envs\\mchatbot\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:676: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:  \n",
      "    Answer the following question based on the context provided below. Provide a detailed, informative response. \n",
      "    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "    Context: Fever is a common symptom of infection.\n",
      "Allergies are the body's immune response to substances.\n",
      "Treatment for allergies often involves antihistamines.\n",
      "    Question: escape\n",
      "    \n",
      "    Only return the helpful answer below and nothing else.\n",
      "    Helpful answer:\n",
      "     You can return to the original question. If the question is not answered, you can ask the person who answered it. The person will respond with a brief explanation of the situation. You may also ask for a copy of your medical history. This information will help you determine if you have a medical condition. It is important to remember that the information you provide is\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load pre-trained model and tokenizer from Hugging Face\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "# Use CPU to avoid memory issues\n",
    "try:\n",
    "    model = AutoModel.from_pretrained(model_name).to('cpu')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    print(f\"Model and tokenizer loaded successfully on CPU.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model or tokenizer: {e}\")\n",
    "    raise\n",
    "\n",
    "# Example text (replace with your text chunks)\n",
    "text_chunks = [\n",
    "    {\"page_content\": \"Allergies are the body's immune response to substances.\"},\n",
    "    {\"page_content\": \"A headache can occur from various conditions, including allergies.\"},\n",
    "    {\"page_content\": \"Fever is a common symptom of infection.\"},\n",
    "    {\"page_content\": \"Skin rashes may appear when someone has an allergy.\"},\n",
    "    {\"page_content\": \"Treatment for allergies often involves antihistamines.\"}\n",
    "]\n",
    "\n",
    "# Generate embeddings for the text chunks\n",
    "def generate_embeddings(text_chunks):\n",
    "    embeddings = []\n",
    "    for chunk in text_chunks:\n",
    "        sentence = chunk['page_content']\n",
    "        inputs = tokenizer(sentence, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "        # Get embeddings\n",
    "        with torch.no_grad():\n",
    "            embedding = model(**inputs).last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "        embeddings.append(embedding)\n",
    "    return np.array(embeddings)\n",
    "\n",
    "# Get embeddings for the text chunks\n",
    "embeddings = generate_embeddings(text_chunks)\n",
    "\n",
    "# Perform similarity search\n",
    "def similarity_search(query, embeddings, text_chunks, top_k=3):\n",
    "    # Generate embedding for the query\n",
    "    inputs = tokenizer(query, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        query_embedding = model(**inputs).last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "    # Compute cosine similarities between query and document embeddings\n",
    "    similarities = cosine_similarity([query_embedding], embeddings)\n",
    "    \n",
    "    # Get indices of the top_k most similar documents\n",
    "    top_k_indices = similarities.argsort()[0][-top_k:][::-1]\n",
    "    \n",
    "    # Get the corresponding documents\n",
    "    results = [text_chunks[i] for i in top_k_indices]\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Function to generate an answer from context using a model (adjusted for quality)\n",
    "def generate_answer(context, question):\n",
    "    # Construct a better prompt that encourages more meaningful answers\n",
    "    prompt = f\"\"\"\n",
    "    Answer the following question based on the context provided below. Provide a detailed, informative response. \n",
    "    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "    Context: {context}\n",
    "    Question: {question}\n",
    "    \n",
    "    Only return the helpful answer below and nothing else.\n",
    "    Helpful answer:\n",
    "    \"\"\"\n",
    "    \n",
    "    # Example: Use GPT-2 for answering (can replace with another model)\n",
    "    gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to('cpu')\n",
    "    gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "    # Adjust temperature and use top-p sampling to control repetition and make the output more varied\n",
    "    inputs = gpt2_tokenizer(prompt, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    \n",
    "    # Generate with adjusted temperature and top-p\n",
    "    outputs = gpt2_model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=200,  # Allow for longer responses\n",
    "        temperature=0.7,  # Less repetitive, more creative\n",
    "        top_p=0.9,  # Top-p sampling to prevent overly deterministic output\n",
    "        num_return_sequences=1,\n",
    "        no_repeat_ngram_size=2,  # Prevent repetition of n-grams\n",
    "        early_stopping=True\n",
    "    )\n",
    "    \n",
    "    # Decode and clean the output\n",
    "    answer = gpt2_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return answer\n",
    "\n",
    "# Main query function to get the helpful answer\n",
    "def get_helpful_answer(query):\n",
    "    # Retrieve relevant documents (context) based on the query\n",
    "    results = similarity_search(query, embeddings, text_chunks)\n",
    "    \n",
    "    # Format the context for the prompt (combine the text from the top results)\n",
    "    context = \"\\n\".join([result['page_content'] for result in results])\n",
    "    \n",
    "    # Generate the answer from the context\n",
    "    try:\n",
    "        response = generate_answer(context, query)\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating response: {e}\")\n",
    "        return \"Sorry, there was an error processing your request.\"\n",
    "\n",
    "# Main loop to query and get answers\n",
    "while True:\n",
    "    user_input = input(\"Input Prompt: \")\n",
    "    result = get_helpful_answer(user_input)\n",
    "    print(\"Response: \", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\anaconda3\\install\\envs\\mchatbot\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded successfully on CPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\anaconda3\\install\\envs\\mchatbot\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ASUS\\anaconda3\\install\\envs\\mchatbot\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.85` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ASUS\\anaconda3\\install\\envs\\mchatbot\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:676: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:  \n",
      "    Answer the following question based on the context provided below. Provide a clear, informative, and concise response.\n",
      "    Avoid unnecessary details and repetition. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "    Context: Skin rashes may appear when someone has an allergy.\n",
      "Allergies are the body's immune response to substances.\n",
      "A headache can occur from various conditions, including allergies.\n",
      "    Question: what is acne?\n",
      "    \n",
      "    Only return the helpful answer below and nothing else.\n",
      "    Helpful answer:\n",
      "     Skin rash is a common skin condition. It is caused by a combination of factors, such as:  a combination or combination acne,    erythema, or ichthyosis.  The skin rash can be caused from: a) a lack of sun protection\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load pre-trained model and tokenizer from Hugging Face\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "# Use CPU to avoid memory issues\n",
    "try:\n",
    "    model = AutoModel.from_pretrained(model_name).to('cpu')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    print(f\"Model and tokenizer loaded successfully on CPU.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model or tokenizer: {e}\")\n",
    "    raise\n",
    "\n",
    "# Example text (replace with your text chunks)\n",
    "text_chunks = [\n",
    "    {\"page_content\": \"Allergies are the body's immune response to substances.\"},\n",
    "    {\"page_content\": \"A headache can occur from various conditions, including allergies.\"},\n",
    "    {\"page_content\": \"Fever is a common symptom of infection.\"},\n",
    "    {\"page_content\": \"Skin rashes may appear when someone has an allergy.\"},\n",
    "    {\"page_content\": \"Treatment for allergies often involves antihistamines.\"}\n",
    "]\n",
    "\n",
    "# Generate embeddings for the text chunks\n",
    "def generate_embeddings(text_chunks):\n",
    "    embeddings = []\n",
    "    for chunk in text_chunks:\n",
    "        sentence = chunk['page_content']\n",
    "        inputs = tokenizer(sentence, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "        # Get embeddings\n",
    "        with torch.no_grad():\n",
    "            embedding = model(**inputs).last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "        embeddings.append(embedding)\n",
    "    return np.array(embeddings)\n",
    "\n",
    "# Get embeddings for the text chunks\n",
    "embeddings = generate_embeddings(text_chunks)\n",
    "\n",
    "# Perform similarity search\n",
    "def similarity_search(query, embeddings, text_chunks, top_k=3):\n",
    "    # Generate embedding for the query\n",
    "    inputs = tokenizer(query, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        query_embedding = model(**inputs).last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "    # Compute cosine similarities between query and document embeddings\n",
    "    similarities = cosine_similarity([query_embedding], embeddings)\n",
    "    \n",
    "    # Get indices of the top_k most similar documents\n",
    "    top_k_indices = similarities.argsort()[0][-top_k:][::-1]\n",
    "    \n",
    "    # Get the corresponding documents\n",
    "    results = [text_chunks[i] for i in top_k_indices]\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Function to generate an answer from context using a model (adjusted for quality)\n",
    "def generate_answer(context, question):\n",
    "    # Construct a better prompt that encourages meaningful answers and clarifies the response\n",
    "    prompt = f\"\"\"\n",
    "    Answer the following question based on the context provided below. Provide a clear, informative, and concise response.\n",
    "    Avoid unnecessary details and repetition. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "    Context: {context}\n",
    "    Question: {question}\n",
    "    \n",
    "    Only return the helpful answer below and nothing else.\n",
    "    Helpful answer:\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use GPT-2 for answering (can replace with another model)\n",
    "    gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to('cpu')\n",
    "    gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "    # Adjust temperature and use top-p sampling to control repetition and make the output more varied\n",
    "    inputs = gpt2_tokenizer(prompt, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    \n",
    "    # Generate with adjusted temperature and top-p\n",
    "    outputs = gpt2_model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=200,  # Allow for longer responses\n",
    "        temperature=0.7,  # Less repetitive, more creative\n",
    "        top_p=0.85,  # Top-p sampling to prevent overly deterministic output\n",
    "        num_return_sequences=1,\n",
    "        no_repeat_ngram_size=2,  # Prevent repetition of n-grams\n",
    "        early_stopping=True\n",
    "    )\n",
    "    \n",
    "    # Decode and clean the output\n",
    "    answer = gpt2_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Check if the answer is meaningful; if it’s too vague, fallback to a default message\n",
    "    if \"I don't know\" in answer or len(answer.split()) < 3:\n",
    "        return \"Sorry, I don't know the answer.\"\n",
    "    \n",
    "    return answer\n",
    "\n",
    "# Main query function to get the helpful answer\n",
    "def get_helpful_answer(query):\n",
    "    # Retrieve relevant documents (context) based on the query\n",
    "    results = similarity_search(query, embeddings, text_chunks)\n",
    "    \n",
    "    # Format the context for the prompt (combine the text from the top results)\n",
    "    context = \"\\n\".join([result['page_content'] for result in results])\n",
    "    \n",
    "    # Generate the answer from the context\n",
    "    try:\n",
    "        response = generate_answer(context, query)\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating response: {e}\")\n",
    "        return \"Sorry, there was an error processing your request.\"\n",
    "\n",
    "# Main loop to query and get answers\n",
    "while True:\n",
    "    user_input = input(\"Input Prompt: \")\n",
    "    result = get_helpful_answer(user_input)\n",
    "    print(\"Response: \", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
